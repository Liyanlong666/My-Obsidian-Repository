# InfluxDB 合并机制与性能测试

## 1. 合并 (Compaction) 机制详解

### 1.1 合并级别 (Level 1-4)
TSM 文件采用分层合并，类似于 LSM 树。
*   **Level 1 (Snapshots)**: Cache 和 WAL 转换而来的原始 TSM 文件。
*   **Level 2-3**: 多个低级别文件合并而成。合并时会去除重复数据、处理删除标记。
*   **Level 4 (Full Compaction)**: 最终状态。文件达到最大尺寸（通常为 2GB），数据高度重组。
*   **策略差异**: 
    *   **低级别**: 尽量避免 CPU 密集的解压重组，侧重于减少文件数量。
    *   **高级别**: 执行全量合并，重新压缩 Block 以获得最高压缩比。

### 1.2 关键配置参数
| 参数                                 | 默认值    | 描述                                        |
| ---------------------------------- | ------ | ----------------------------------------- |
| `cache-snapshot-memory-size`       | 25MB   | 达到此值触发 Snapshot 为 L1 文件。                  |
| `compact-full-write-cold-duration` | 4h     | 若 Shard 在 4 小时内没新数据写入，执行 Full Compaction。 |
| `max-concurrent-compactions`       | 0 (自动) | 同时运行的合并任务数。                               |
| `compact-throughput`               | 48MB/s | 限制合并时的磁盘 IO，防止影响查询。                       |

### 1.3 索引优化 (Index Optimization)
当 L4 文件积累过多，TSM 内部索引会因为包含过多重复 SeriesKey 而膨胀。
*   **操作**: 将不同文件中属于同一 Series 的数据重新排序并写到一起。
*   **结果**: 显著减小内存中加载的索引体积。

---

## 2. 性能测试 (Benchmark)

### 2.1 写入吞吐压测 (influx-stress)
测试命令：`influx-stress insert -pps 200000 --host http://localhost:8086`

| 目标 PPS (pts/s) | 实际处理 (pts/s) | CPU 平均利用率 |
| --- | --- | --- |
| 200,000 | 199,713 | 33% |
| 500,000 | 491,135 | 80% |
| 600,000 | 593,542 | 90% |
| 700,000 | **613,791 (瓶颈)** | 95% |
*   **结论**: 写入峰值在 **60万/秒** 左右，建议保留 20% CPU 余量，日常负载应控制在 40万/秒内。

### 2.2 查询延迟压测 (influxdb-comparisons)
基于 1000 个 Host，单表 2000+ 行数据的聚合查询测试：

| 查询次数 (-limit) | 平均执行时间 | 最大执行时间 |
| --- | --- | --- |
| 100 | 1.69 ms | 4.36 ms |
| 500 | 1.70 ms | 7.54 ms |
| 1000 | 1.64 ms | 8.33 ms |
*   **结论**: 在标准聚合查询场景下，单次查询响应在 **2ms** 以内，并发能力强。

---

## 3. 运维与最佳实践
*   **批量写入**: 最佳批次为 5,000 - 10,000 个 Point。
*   **Tag 基数**: 严控 Tag 数量。Tag 会建立倒排索引，基数（Cardinality）过大会导致内存爆炸。
*   **冷热分离**: 利用 RP 自动清理历史数据，配合 Full Compaction 减少长期存储开销。

---
*整理自：TSM 文件的合并时机、influxDB性能测试 相关文档。*

